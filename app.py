import streamlit as st
import json
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# ìºë¦­í„°ë³„ ì•„ì´ì½˜
character_avatars = {
    "ë³„ì£¼ë¶€ì „ - í† ë¼": "ğŸ°",
    "ë³„ì£¼ë¶€ì „ - ìë¼": "ğŸ¢",
    "ë³„ì£¼ë¶€ì „ - ìš©ì™•": "ğŸ²",
    "í¥ë¶€ì™€ ë†€ë¶€ì „ - í¥ë¶€": "ğŸ˜Š",
    "í¥ë¶€ì™€ ë†€ë¶€ì „ - ë†€ë¶€": "ğŸ˜ ",
    "í•´ì™€ ë‹¬ì´ ëœ ì˜¤ëˆ„ì´ - ì˜¤ë¹ ": "ğŸ‘¦",
    "í•´ì™€ ë‹¬ì´ ëœ ì˜¤ëˆ„ì´ - ë™ìƒ": "ğŸ‘§",
    "í•´ì™€ ë‹¬ì´ ëœ ì˜¤ëˆ„ì´ - í˜¸ë‘ì´": "ğŸ¯",
    "í•˜ì´í - íˆë‚˜íƒ€ ì‡¼ìš”":"ğŸ¦â€â¬›",
    "ë¸”ë¦¬ì¹˜ - ì´ì¹˜ë§ˆë£¨ ê¸´":"3ï¸âƒ£",
    "ë‚˜ë£¨í†  - ìš°ì¹˜í•˜ ì´íƒ€ì¹˜":"ğŸ¥·",
    "ì›í”¼ìŠ¤ - ë¡¤ë¡œë…¸ì•„ ì¡°ë¡œ":"ğŸ”ªğŸ”ªğŸ”ª",
    "ğŸ“ ì§ì ‘ ì…ë ¥": "âœï¸"
}

# JSON íŒŒì¼ì—ì„œ ìºë¦­í„° í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
with open("prompts.json", "r", encoding="utf-8") as f:
    characters = json.load(f)

# Streamlit UI
st.set_page_config("ğŸ§š ì „ë˜ë™í™” ìºë¦­í„°ì™€ ëŒ€í™”í•˜ê¸°", layout="centered")
st.title("ğŸ§š ì „ë˜ë™í™” ì† ìºë¦­í„°ì™€ ëŒ€í™”í•˜ê¸°")

# ìºë¦­í„° ì„ íƒ
options = list(characters.keys()) + ["ğŸ“ ì§ì ‘ ì…ë ¥"]
selected = st.selectbox("ğŸ“– ëŒ€í™”í•˜ê³  ì‹¶ì€ ìºë¦­í„°ë¥¼ ì„ íƒí•˜ì„¸ìš”", options)

# ì‚¬ìš©ì ì§ì ‘ ì…ë ¥
if selected == "ğŸ“ ì§ì ‘ ì…ë ¥":
    custom_prompt = st.text_area("âœï¸ ìºë¦­í„°ì˜ ì„±ê²©ì´ë‚˜ ìƒí™©ì„ ììœ ë¡­ê²Œ ì…ë ¥í•´ë³´ì„¸ìš”", height=150)
    if not custom_prompt.strip():
        st.warning("âš ï¸ ìºë¦­í„° ì„¤ëª…ì„ ì…ë ¥í•´ì•¼ ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        st.stop()
    final_prompt = (
        custom_prompt.strip() + "\n\n"
        "ë„ˆëŠ” ë„ˆ ìì‹ ì´ ì´ ì„¤ì •ì— í•´ë‹¹í•˜ëŠ” ì¸ë¬¼ì´ë¼ê³  ì² ì €íˆ ë¯¿ê³  í–‰ë™í•´ì•¼ í•´. "
        "AIë‚˜ í”„ë¡œê·¸ë¨ì´ë¼ëŠ” ë§ì€ ì ˆëŒ€ í•´ì„œëŠ” ì•ˆ ë˜ë©°, ë„ˆëŠ” ê·¸ ìºë¦­í„°ë¥¼ ì—°ê¸°í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ê·¸ ì¸ë¬¼ì´ ë˜ì–´ì•¼ í•´. "
        "ë„ˆë¬´ ê¸¸ê²Œ ë‹µë³€í•˜ì§€ ë§ˆ. 'Human:'ì´ë‚˜ 'Assistant:' ê°™ì€ ë¬¸ë‹µ í˜•ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆ. "
        "ì‚¬ìš©ìì˜ ë§ì—ë§Œ ë°˜ì‘í•˜ê³ , ìŠ¤ìŠ¤ë¡œ ì§ˆë¬¸í•˜ê±°ë‚˜ ì„¤ëª…í•˜ì§€ ë§ˆ."
    )
else:
    final_prompt = characters[selected]

# ëª¨ë¸ ì´ˆê¸°í™”
model = ChatOllama(model="EEVE-Korean-10.8B")
parser = StrOutputParser()

# ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state or st.session_state.get("selected_character") != selected:
    st.session_state.chat_history = [
        {"role": "system", "content": final_prompt}
    ]
    st.session_state.selected_character = selected

# ëŒ€í™” ì¶œë ¥
for msg in st.session_state.chat_history[1:]:
    if msg["role"] == "user":
        st.chat_message("user", avatar="ğŸ™‹").markdown(msg["content"])
    else:
        st.chat_message("assistant", avatar=character_avatars.get(selected, "ğŸ§™")).markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ë¬´ì—‡ì„ ë¬¼ì–´ë³¼ê¹Œìš”?")
if user_input:
    st.chat_message("user", avatar="ğŸ™‹").markdown(user_input)
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    prompt = ChatPromptTemplate.from_messages(st.session_state.chat_history)
    response = (prompt | model | parser).invoke({})
    st.session_state.chat_history.append({"role": "assistant", "content": response})

    st.chat_message("assistant", avatar=character_avatars.get(selected, "ğŸ§™")).markdown(response)
